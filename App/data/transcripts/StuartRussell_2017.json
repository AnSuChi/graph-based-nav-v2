{
  "id": "StuartRussell_2017",
  "transcript": "one of the world 's greatest go players and he 's having what my friends in silicon valley call a holy cow moment a moment where we realize that ai is actually progressing a lot faster than we expected the real world is much bigger much more complicated than the go board it and if we think about some of the technologies that are coming down the pike but that will happen and when that happens very soon afterwards machines will have read and that will enable machines along with the ability to look further ahead than humans can as we 've already seen in go if they also have access to more information they 'll be able to make better decisions in the real world than we can so is that a good thing i hope so so why are people saying things like this that ai might spell the end of the human race is it just elon musk and bill gates and stephen hawking actually no this idea has been around for a while even if we could keep the machines in a subservient position for instance by turning off the power at strategic moments and i 'll come back to that turning off the power idea later on we should as a species feel greatly humbled so who said this this is alan turing in so if we think about this problem problem of creating something more intelligent than your own species we might call this the gorilla problem and now we can ask the gorillas so here they are having a meeting to discuss whether it was a good idea and after a little while they conclude no this was a terrible idea our species is in dire straits in fact you can see the existential sadness in their eyes so this queasy feeling that making something smarter than your own species is is maybe not a good idea except stop doing ai and because of all the benefits that i mentioned and because i 'm an ai researcher i 'm not having that i actually want to be able to keep doing ai so here 's another quotation we had better be quite sure that the purpose put into the machine is the purpose which we really desire this was said by norbert wiener but this could equally have been said by king midas so to speak then his food and his drink and his relatives turned to gold and he died in misery and starvation so we 'll call this the king midas problem truly aligned with what we want in modern terms we call this the value alignment problem putting in the wrong objective is not the only part of the problem there 's another part if you put an objective into a machine even something as simple as fetch the coffee the machine says to itself i will do anything to defend myself against interference with this objective that i have been given single minded pursuit not aligned with the true objectives of the human race and in fact that 's the it 's that you can 't fetch the coffee if you 're dead simple just remember that repeat it to yourself three times a day and in fact this is exactly the plot of two thousand and one [ a space odyssey ] hal has an objective a mission which is not aligned with the objectives of the humans and that leads to this conflict now fortunately so what are we going to do i 'm trying to redefine ai to get away from this classical notion of machines that intelligently pursue objectives there are three principles involved the first one is a principle of altruism if you like that the robot 's only objective is to maximize the realization of human objectives of human values and by values here i don 't mean values i just mean whatever it is that the human would prefer their life to be like but it doesn 't know what they are now in order to be useful to us it has to have some idea of what we want it obtains that information primarily by observation of human choices so our own choices reveal information so here 's a pr2 robot this is one that we have in our lab and it has a big red off switch i can 't fetch the coffee and so it says therefore i must disable my ' off ' switch and probably taser all the other people in starbucks who might interfere with me so what happens if the machine is uncertain about the objective well it reasons in a different way it says ok the human might switch me off but only if i 'm doing something wrong well i don 't really know what wrong is but i know that i don 't want to do it so that 's the first and second principles right there i should let the human switch me off and then when the machine that third principle comes it learns something about the objectives it should be pursuing because it learns that what it did wasn 't right we can actually prove a theorem that says that such a robot is provably beneficial to the human you are provably better off with a machine that 's designed in this way than without it so this is a very simple example but this is the first step in in what we 're trying to do with now i don 't want my my robot to behave like me i sneak down in the middle of the night and take stuff from the fridge i do this and that there 's all kinds of things you don 't want the robot doing but in fact it doesn 't quite work that way just because you behave badly doesn 't mean the robot is going to copy your behavior it 's going to understand your motivations is to allow machines to predict for any person and the lives of everybody else which would they prefer and there are many many difficulties involved in doing this i don 't expect that this is going to get solved very quickly we behave badly in fact some of us are downright nasty so it can deal with a certain amount of nastiness and it can even understand that your nastiness for you may take bribes as a passport official because you need to feed your family and send your kids to school in fact it 'll just help you send your kids to school we are also computationally limited that doesn 't mean he wanted to lose so to understand his behavior probably the most difficult part from my point of view as an ai researcher is the fact that there are lots of us and so the machine has to somehow trade off weigh up the preferences of many different people moral philosophers have understood that and we are actively looking for collaboration let 's have a look and see what happens when you get that wrong so you can have a conversation for example with your intelligent personal assistant that might be available in a few years ' time think what are you talking about i arranged for his plane to be delayed some kind of computer malfunction really you can do that he sends his profound apologies and looks forward to meeting you for lunch tomorrow this is clearly following my wife 's values which is happy wife happy life you could come home after a hard day 's work there are humans in south sudan who are in more urgent need than you so i 'm leaving make your own dinner there are reasons for optimism one reason is there is a massive amount of data because remember i said they 're going to read everything the human race has ever written most of what we write about is human beings doing things and other people getting upset about it so there 's a massive amount of data to learn from there 's also a very strong economic incentive so imagine your domestic robot 's at home you 're late from work again and the robot sees the cat it happens like this deranged robot cooks kitty for family dinner so there 's a huge incentive to get this right so to summarize but that are uncertain about what those objectives are and will watch all of us to learn more about what it is that we really want thank you very much chris anderson so interesting stuart we 're going to stand here a bit because i think they 're setting up for our next speaker a couple of questions as you get to what 's going to stop a robot reading literature and discovering this idea that knowledge is actually better than ignorance and still just shifting its own goals and rewriting that as it becomes more and it 's going to be designed to interpret it correctly it will understand for example that books so it 's a complicated problem i comply i comply sr that would be a terrible idea so imagine that you have a self driving car and you want to send your five year old off to preschool to be able to switch off the car while it 's driving along probably not so it needs to understand the more willing you are to be switched off if the person is completely random or even malicious then you 're less willing to be switched off ca all right stuart can i just say i really really hope you figure this out for us thank you so much for that"
}