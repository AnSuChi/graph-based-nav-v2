{
  "id": "ZeynepTufekci_2016T",
  "transcript": "soon after i started working writing software in a company a manager who worked at the company came down to where i was and he whispered to me can he tell if i 'm lying there was nobody else in the room can who tell if you 're lying and why are we whispering the manager pointed at the computer in the room can he that manager was having an affair with the receptionist so i whisper shouted back to him yes the computer can tell if you 're lying well there are computational systems that can suss out emotional states and even lying from advertisers and even governments are very interested so i thought to myself hey let me pick a technical field where i can so i picked computers nowadays computer scientists are building platforms that control what they 're developing cars that could decide who to run over even building machines weapons that might kill human beings in war it 's ethics all the way down machine intelligence is here we we 're asking questions to computation that have no we 're asking questions like who should the company hire which update from which friend should you be shown which convict is more likely to reoffend which news item or movie should be recommended to people yes we 've been using computers for a while but this is different this is a historical twist because we cannot anchor computation for such subjective decisions the way we can anchor computation for flying airplanes building bridges going to the moon are airplanes safer did the bridge sway and fall there we have agreed upon fairly clear benchmarks and we have laws of nature to guide us we have no such anchors and benchmarks for decisions in messy but it 's also getting less transparent and more complex they can detect credit card fraud and block spam and they can translate between languages they can they can beat humans in chess and go much of this progress comes from a method called machine learning it 's more like you take the system and you feed it lots of data including unstructured data like the kind we generate in our digital lives and the system learns by churning through this data they don 't produce a simple answer it 's more probabilistic this one is probably more like what you 're looking for now the upside is this method is really powerful the head of google 's ai systems called it the unreasonable effectiveness of data downside is we don 't really understand what the system learned in fact that 's its power this is less like giving instructions to a it 's more like training it 's a problem when this artificial intelligence system gets things wrong we don 't know what this thing is thinking consider a hiring algorithm using machine learning systems such a system would have been trained on previous employees ' data and instructed to find and hire people like the existing high performers in the company sounds good they thought that this would make hiring more objective give women and minorities a better shot against biased human managers and look human hiring is biased i know i mean in one of my early jobs as a programmer my immediate manager would sometimes come down to where i was really early in the morning or really late in the afternoon and she 'd say zeynep let 's go to lunch i later realized what was happening my immediate managers had not confessed to their higher ups that the programmer they hired for a serious job was a teen girl i was doing a good job i just looked wrong and was the wrong age they can infer they have predictive power i have a friend who developed such computational systems to predict the likelihood of clinical or postpartum depression from social media data the results are impressive her system can predict the likelihood of depression months before the onset of any symptoms months before no symptoms there 's prediction she hopes it will be used for so at this human resources managers conference i approached a high level manager in a very large what if unbeknownst to you your system they 're not depressed now just maybe in the future more likely what if it 's weeding out women more likely to be pregnant in the next year or two but aren 't pregnant now what if it 's hiring aggressive people because that 's your workplace culture and since this is machine learning not traditional coding there is no variable there labeled higher risk of depression higher risk of pregnancy aggressive guy scale not only do you not know what your system is selecting but you don 't understand it what safeguards i asked do you have to make sure that your black box isn 't doing something shady she looked at me as if i had just stepped on ten puppy tails laughter she stared at me and she said i don 't want to hear another word about this death stare but it could also lead to a steady but stealthy shutting out of the job market of people with higher risk of depression decision making to machines we don 't totally understand another problem is this these systems are often trained on data generated by our actions could just be reflecting our biases and these systems could be picking up on our biases and amplifying them and showing them back to us while we 're telling ourselves we 're just doing objective neutral computation researchers found that on google women are less likely than men to be shown job ads for high paying jobs and searching for african american names is more likely to bring up ads suggesting criminal history even when there is none such hidden biases and black box algorithms a defendant was sentenced to six years in prison for evading the police it 's a commercial black box the company refused to have its algorithm be challenged in open court but propublica an investigative nonprofit audited that very algorithm with what public data they could find and it was wrongly labeling black defendants as future criminals at twice the rate of white defendants that 's my kid 's bike she was wrong she was foolish but she was also just eighteen she eighty five dollars ' worth of stuff a similar petty crime but the algorithm scored her as high risk and not him two years later propublica found that she had not reoffended was just hard to get a job for her with her record he on the other hand did reoffend and is now serving an eight year prison term for a later crime we need to audit our black boxes and not have them have this kind of unchecked power audits are great and important but they don 't solve all our problems take facebook 's powerful news feed algorithm you know the one that ranks everything and decides show you from all the friends and pages you follow should you be shown another baby picture there 's no right answer facebook optimizes for engagement on the site likes shares comments in august of two thousand and fourteen protests broke out in ferguson missouri after the killing the news of the protests was all over my algorithmically unfiltered twitter feed but nowhere on my facebook was it my facebook friends i disabled facebook 's algorithm which is hard because facebook keeps wanting to make you come under the algorithm 's control and saw that my friends were talking about the story of ferguson wasn 't it 's not likable who 's going to click on like it 's not even easy to comment on without likes and comments the algorithm was likely showing it to even fewer people get to see this but it was super algorithm friendly the machine made this decision for us a very important but difficult conversation might have been smothered had facebook been the only these systems can also be wrong in ways that don 't resemble human systems do you guys remember watson watson was asked this question its largest airport is named for a world war ii hero its second largest for a world war ii battle the impressive system also made an error that a human would never make a second grader wouldn 't make our machine intelligence can fail in ways that don 't fit error patterns of humans in ways we won 't expect and be prepared for but it would triple suck if it was because of stack overflow in some subroutine laughter in may of two thousand and ten a flash crash on wall street fueled by a feedback loop in wall street 's sell algorithm wiped a trillion dollars of so yes humans have always made biases decision makers and gatekeepers in courts in news they make mistakes but that 's exactly my point we cannot escape these difficult questions we cannot outsource our responsibilities to machines artificial intelligence does not give us a get out of ethics free card we need we need to accept that bringing math and computation to messy human affairs does not bring objectivity rather the complexity of human affairs invades the algorithms we can and we should use computation to help us make better decisions but have to own up to our moral responsibility to judgment and use algorithms within that framework not as a means to abdicate and outsource our responsibilities to one another as human to human"
}