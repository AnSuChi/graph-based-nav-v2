{
  "id": "EdUlbrich_2009",
  "transcript": "i 'm here today representing a team of artists and technologists and filmmakers that worked together on a remarkable film project for the last four years so i want to show you a clip of the film now hopefully it won 't stutter you won 't know that we were even involved there were many changes some you could i felt pretty good considering ed ulbrich maybe you 've seen it or you 've heard of the story but what you might not know is that for nearly the first hour of the film the main character benjamin button who 's played by brad pitt is completely computer generated from the neck up we 've created a completely digital human head so i 'd like to start with a little bit of history on the project this is based on an f scott fitzgerald short story it 's about a man who 's born old and lives his life in reverse now this movie has floated around hollywood for well over half a century and we first got involved with the project in the early ' 90s with ron howard as the director we took a lot of meetings and it was deemed impossible it was beyond the technology of the day to depict a man aging backwards backwards the human form in particular the human head has been considered the holy grail of our industry fincher is is an interesting guy and david won 't take no and david believed like we do that anything is possible as long as you have enough resources and of course money we went through a process of elimination and a process of discovery with david and we ruled out of course swapping actors that was one idea that we would have different actors and we would hand off from actor to actor we even ruled out the idea of using makeup we realized that prosthetic he needed to be a very sympathetic character so we decided to cast a series of little people that would play the different bodies of benjamin at the different increments of his life and that we would in fact create a and the fact that this guy is a global icon didn 't help you know we see his face constantly so there really was no tolerable margin of error and they both believed this would make an amazing film of course but it was a very high risk proposition there was lots of money and reputations at stake but we believed that we had a very solid methodology that but despite our verbal assurances they wanted some proof and we did it in about five and in here you can see that 's a computer generated head and it gave the studio great relief after many years of starts and stops on this project and making that tough decision they finally decided to greenlight the movie and i can remember actually when i got the phone call to congratulate us to say the movie was a go i actually threw up you so we started to have early team meetings and we got everybody together and it was really more like therapy in the beginning convincing each other and reassuring each other that we could actually undertake this we had to hold up an hour of a movie with a character and it 's not a special effects film it has to be a man and of course the first step is admit you 've got a problem so but we did know one thing being from the visual effects industry we with david believed that we now had enough time enough resources and god we hoped we had enough money when you 're faced with something like that of course you 've got to break it down you take the big problem and you break it down into smaller pieces and you start to attack that so we had three main areas that we had to focus on we needed to make brad look a lot older needed to age and we also needed to make sure that we could take brad 's and we also needed to create he needed to be able to walk in broad daylight at nighttime under candlelight he had to hold an extreme close up he had to deliver dialogue he had to be able to run he had to be able to sweat he had to be able to take a bath to cry he even had to throw up not all at the same time but he had to you know and we realized that there was a giant chasm between the state of the art of technology in two thousand and four and where we needed it to be the state of the art at the time was something called marker based motion capture i 'll give you an example here it 's basically the idea of you wear a leotard and those infrared sensors track the in real time and then animators can take the data of the motion of those markers and apply them to a computer generated character you can see the computer characters on the right are having the same complex motion as the as the dancers but we also looked at numbers of other films at the time that were using facial marker tracking and that 's the idea of putting markers on the human face and doing the same process and as you can see it gives you a pretty crappy performance that and what we realized was that what we needed was the information that was going on between the markers we needed the the subtleties of the skin we needed to see skin moving over muscle moving over bone we needed creases and dimples and wrinkles and all of those things our first revelation was to completely abort and walk away from the technology of the day the status quo and in uncharted territory we were left with this idea that we ended up calling the idea was that we were going to find nuggets or gems of technology that and we had to create kind of a sauce and the sauce was code in software that we 'd written to allow these disparate pieces of technology to come initially we came across some remarkable research done by a gentleman named dr paul ekman in the early ' 70s he believed that he could in fact catalog the human face and he came up with this idea of facial action coding system or facs he believed that there were seventy basic poses or shapes of the and of course so this became the foundation of our research as we went forward and then we came across some remarkable technology called contour and here you can see a subject having phosphorus makeup stippled on her face and now what we 're looking at is really and those cameras can frame by frame reconstruct the geometry of exactly what the subject 's doing at the moment so effectively you get and if you look in a comparison on the left we see what volumetric data gives us and on the right you see what markers give us so clearly we were in a substantially better place for this but these were the early days of this technology we measure and so on the left we were seeing one hundred thousand polygons we could go up into the millions of polygons it seemed to be infinite this was when we had our aha this was the breakthrough this is when we 're like ok we 're going to be ok this is actually going to work and and we could put brad in this device and we could in fact scan him in real time performing ekman 's facs poses effectively we ended up with a 3d database of everything brad pitt 's face is capable of doing from there we actually carved up those faces into smaller pieces and components of his face so we ended up with literally thousands and thousands and thousands of shapes that 's great except we had him at age forty four we need to put another forty years on him at this point we and we also brought in a gentleman named kazu tsuji and kazu tsuji is one of the great and we commissioned them to make a so this is ben eighty and this really became this was made from a life cast of brad so in fact anatomically it is correct the eyes the jaw the teeth everything is in perfect alignment with what the real guy has we have these maquettes scanned into the computer at at very high resolution enormous polygonal count and so now we had but we needed to get a database of him doing more than that we went through this process then called retargeting this is brad doing one of the ekman facs poses and here 's the resulting data that comes from that the model that comes from that retargeting is the process of transposing that data onto another model and because the life cast or the bust the maquette of benjamin was made from brad we could transpose the data everything brad pitt 's face can do at age eighty seven in his 70s and in his 60s next we had to go into the shooting process so while all that 's going on we 're down in new orleans and locations around the world and we shot our body actors and we shot them wearing blue hoods so these are the gentleman who played benjamin and so we edited the footage that was shot on location with the the rest of the cast and the body actors and about six months later we brought brad onto a sound stage in los angeles and he watched on the screen and he took benjamin into interesting and unusual places that we didn 't think he was going to go we shot him with four hd cameras so we 'd get multiple views of him and then david would choose the take of brad being benjamin that he thought best matched the footage with the rest of the cast and you are seeing now that data being transposed on to ben about this is we used something called image analysis which is taking timings from different components of benjamin 's face and so we could choose say his left eyebrow and the software would tell us that well in frame fourteen the left eyebrow begins to move from here to here and it concludes moving in frame thirty two and so we could choose numbers of positions on the face to pull that data from and then the sauce i talked about with our technology stew that secret sauce to match the performance footage of brad in live action with our database of aged benjamin the facs shapes that we had on a frame by frame basis we could actually reconstruct and then this is what we called the dead head no reference to jerry garcia and the final shot it was a long process i 'm going to just blast through this because we could do a whole tedtalk on the next we had to create a lighting system so really a big part of our processes was creating a lighting environment for every single location that benjamin had to appear so that we found and if you could feel the warmth and feel the humanity and feel his intent coming through the we worked from dental molds of brad we had to age the teeth over time we also had to create an articulating tongue that allowed him to enunciate his words there was a whole system written in software to articulate the tongue we had one person devoted to the tongue for about nine months he was very popular lots of work on skin deformation you can see in some of these cases it works in some cases it looks bad this is a very very very early test in there were no animators necessary to come in and interpret behavior or fincher would always say it sandblasts the edges off of the performance so it sees a smile as a smile it doesn 't recognize an ironic smile or a happy smile or a frustrated smile so it did take humans to kind of push it one way or another but we ended up calling the entire process and all the technology emotion capture as opposed to just motion capture take another look"
}